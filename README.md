# üè¶ Banking Analysis Suite

–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AI –º–æ–¥–µ–ª–µ–π.

## üéØ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### üí≥ OperCode Prediction
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–æ–≤ –æ–ø–µ—Ä–∞—Ü–∏–π –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –ø–ª–∞—Ç–µ–∂–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º fine-tuned –º–æ–¥–µ–ª–∏ **Gemma 2 9B**.

### üë§ Name Extraction (NER)
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω –∏ —Ñ–∞–º–∏–ª–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ **Qwen 2.5 14B** —á–µ—Ä–µ–∑ Ollama.

### üîÑ Unified Analysis
–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±–µ–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

**Hardware:**
- GPU: NVIDIA —Å 8+ GB VRAM
- RAM: 16+ GB
- –î–∏—Å–∫: 30+ GB —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞

**Software:**
- Python 3.10+
- CUDA 11.8+ –∏–ª–∏ 12.x
- Ollama

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞

#### 1. –ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
```bash
git clone <repository-url>
cd RPA
```

#### 2. –°–æ–∑–¥–∞–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
```bash
conda create -n rpa python=3.10 -y
conda activate rpa
```

#### 3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
```bash
# PyTorch —Å CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
pip install transformers accelerate peft bitsandbytes
pip install datasets pandas numpy scikit-learn
pip install fastapi uvicorn pydantic

# –î–ª—è NER
pip install ollama
```

#### 4. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### 5. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è NER
```bash
ollama pull qwen2.5:14b
```

### –ó–∞–ø—É—Å–∫

#### –¢–µ—Ä–º–∏–Ω–∞–ª 1: –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama
```bash
ollama serve
```

#### –¢–µ—Ä–º–∏–Ω–∞–ª 2: –ó–∞–ø—É—Å—Ç–∏—Ç–µ API
```bash
cd api
./start_unified_api.sh
```

#### –û—Ç–∫—Ä–æ–π—Ç–µ –±—Ä–∞—É–∑–µ—Ä
```
http://localhost:8000
```

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
RPA/
‚îú‚îÄ‚îÄ api/                          # API –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã
‚îÇ   ‚îú‚îÄ‚îÄ unified_api_server.py     # –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π API
‚îÇ   ‚îú‚îÄ‚îÄ api_server.py             # OperCode API
‚îÇ   ‚îú‚îÄ‚îÄ START_HERE.md             # –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API
‚îÇ
‚îú‚îÄ‚îÄ gemma_finetuning/             # Fine-tuning Gemma –º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                  # –°–∫—Ä–∏–ø—Ç—ã –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_qlora.py        # –û–±—É—á–µ–Ω–∏–µ —Å QLoRA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py           # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prepare_data.py       # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îú‚îÄ‚îÄ outputs/                  # Checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ data/                     # –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îÇ
‚îú‚îÄ‚îÄ NER/                          # Name Extraction –º–æ–¥—É–ª—å
‚îÇ   ‚îú‚îÄ‚îÄ ner_extraction_ollama.py  # NER –º–æ–¥—É–ª—å
‚îÇ   ‚îú‚îÄ‚îÄ ner_web_interface.py      # Streamlit –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îÇ
‚îú‚îÄ‚îÄ data/                         # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
‚îÇ   ‚îú‚îÄ‚îÄ purpose_codes.txt         # –°–ª–æ–≤–∞—Ä—å –∫–æ–¥–æ–≤ –æ–ø–µ—Ä–∞—Ü–∏–π
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ docs/                         # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ –¥–∞–Ω–Ω—ã–µ
    ‚îî‚îÄ‚îÄ ...
```

---

## üé® –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å

### Unified Web Interface

–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å —Ç—Ä–µ–º—è —Ä–µ–∂–∏–º–∞–º–∏ —Ä–∞–±–æ—Ç—ã:

#### üí≥ OperCode Predictor
```
–í–≤–æ–¥: "PURCHASE OF GOODS"
–†–µ–∑—É–ª—å—Ç–∞—Ç: –ö–æ–¥ 10151 - –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø—Ä–æ–¥—É–∫—Ü–∏—è
```

#### üë§ Name Extractor
```
–í–≤–æ–¥: "–ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤"
–†–µ–∑—É–ª—å—Ç–∞—Ç: –ò–º—è: –ò–≤–∞–Ω, –§–∞–º–∏–ª–∏—è: –ü–µ—Ç—Ä–æ–≤
```

#### üîÑ Unified Analysis
```
–í–≤–æ–¥: "–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤ —Å–æ–≤–µ—Ä—à–∏–ª –ø–µ—Ä–µ–≤–æ–¥ —Å—Ä–µ–¥—Å—Ç–≤ –Ω–∞ —Å–≤–æ–π —Å—á–µ—Ç"
–†–µ–∑—É–ª—å—Ç–∞—Ç:
  - –ö–æ–¥: 40101 (–ü–µ—Ä–µ–≤–æ–¥—ã –Ω–∞ —Å–≤–æ–π —Å—á–µ—Ç)
  - –ò–º—è: –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤
```

---

## ü§ñ –ú–æ–¥–µ–ª–∏

### Gemma 2 9B IT + QLoRA
- **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–æ–≤ –æ–ø–µ—Ä–∞—Ü–∏–π
- **–†–∞–∑–º–µ—Ä:** 18 GB (–Ω–∞ –¥–∏—Å–∫–µ), 6.6 GB VRAM (4-bit)
- **Checkpoint:** checkpoint-3000 (epoch 0.18)
- **–ö–∞—á–µ—Å—Ç–≤–æ:** ~15% accuracy (–≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è)

### Qwen 2.5 14B
- **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω –∏ —Ñ–∞–º–∏–ª–∏–π (NER)
- **–†–∞–∑–º–µ—Ä:** 9 GB (–Ω–∞ –¥–∏—Å–∫–µ), 4 GB VRAM + 9.6 GB RAM
- **–ö–∞—á–µ—Å—Ç–≤–æ:** –û—Ç–ª–∏—á–Ω–æ–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞

---

## üìä API Endpoints

### REST API

```bash
# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ–¥–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏
POST /predict/opercode
{
  "payment_comment": "PURCHASE OF GOODS"
}

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω
POST /predict/ner
{
  "text": "–ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤"
}

# –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
POST /predict/unified
{
  "text": "–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤ —Å–æ–≤–µ—Ä—à–∏–ª –ø–µ—Ä–µ–≤–æ–¥ —Å—Ä–µ–¥—Å—Ç–≤",
  "extract_opercode": true,
  "extract_names": true
}

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è
GET /health

# –°–ø–∏—Å–æ–∫ –∫–æ–¥–æ–≤
GET /codes

# Swagger –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
GET /docs
```

---

## üîß Fine-tuning

### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ OperCode

```bash
cd gemma_finetuning

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
python scripts/prepare_data.py

# –û–±—É—á–µ–Ω–∏–µ
python scripts/train_qlora.py

# –û—Ü–µ–Ω–∫–∞
python scripts/evaluate.py
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è

```bash
# TensorBoard
tensorboard --logdir logs/

# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
python scripts/monitor_training.py
```

–ü–æ–¥—Ä–æ–±–Ω–µ–µ: [gemma_finetuning/README.md](gemma_finetuning/README.md)

---

## üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

| –§—É–Ω–∫—Ü–∏—è | –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å | –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ | –†–µ—Å—É—Ä—Å—ã |
|---------|---------------|-------------|---------|
| OperCode | 10-15 —Å–µ–∫ | 3-5 —Å–µ–∫ | 6.6 GB VRAM |
| NER | 5-10 —Å–µ–∫ | 5-10 —Å–µ–∫ | 4 GB VRAM + 9.6 GB RAM |
| Unified | 15-25 —Å–µ–∫ | 8-15 —Å–µ–∫ | –û–±–∞ |

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤

```
GPU VRAM (12 GB):  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 91%
RAM (32 GB):       [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 25%
CPU:               [‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 4%
```

---

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### –û—Å–Ω–æ–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [api/START_HERE.md](api/START_HERE.md) - –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç API
- [api/README.md](api/README.md) - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API
- [api/UNIFIED_API_README.md](api/UNIFIED_API_README.md) - –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Unified API

### Fine-tuning
- [gemma_finetuning/README.md](gemma_finetuning/README.md) - –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
- [gemma_finetuning/MONITORING_GUIDE.md](gemma_finetuning/MONITORING_GUIDE.md) - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
- [gemma_finetuning/CLASS_BALANCING.md](gemma_finetuning/CLASS_BALANCING.md) - –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤

### NER
- [NER/README.md](NER/README.md) - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è NER –º–æ–¥—É–ª—è

---

## üîê –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å

- ‚úÖ –í—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ
- ‚úÖ –î–∞–Ω–Ω—ã–µ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
- ‚úÖ –ù–µ –Ω—É–∂–Ω—ã API –∫–ª—é—á–∏
- ‚úÖ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –¥–∞–Ω–Ω—ã–º–∏

---

## üêõ –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### NER –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Ollama
ollama list

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama
ollama serve

# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å
ollama pull qwen2.5:14b
```

### CUDA out of memory
```bash
# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –¥—Ä—É–≥–∏–µ GPU –ø—Ä–æ—Ü–µ—Å—Å—ã
nvidia-smi
kill <PID>
```

### –ü–æ—Ä—Ç –∑–∞–Ω—è—Ç
```bash
lsof -ti:8000 | xargs kill -9
```


---

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

- **Google** - –∑–∞ –º–æ–¥–µ–ª—å Gemma
- **Alibaba Cloud** - –∑–∞ –º–æ–¥–µ–ª—å Qwen
- **Hugging Face** - –∑–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
- **Ollama** - –∑–∞ —É–¥–æ–±–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∫ LLM

---

**–°–æ–∑–¥–∞–Ω–æ —Å ‚ù§Ô∏è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π**


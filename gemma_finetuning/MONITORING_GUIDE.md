# üîç –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥—É –æ–±—É—á–µ–Ω–∏—è

## üéØ –ö–∞–∫ –ø–æ–Ω—è—Ç—å, —á—Ç–æ –≤—Å–µ –∏–¥–µ—Ç —Ö–æ—Ä–æ—à–æ?

### ‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ –∑–¥–æ—Ä–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

#### 1. GPU –ú–µ—Ç—Ä–∏–∫–∏
```
‚úÖ GPU Utilization: 70-100%
   - –ï—Å–ª–∏ < 30% ‚Üí –ø—Ä–æ–±–ª–µ–º–∞, GPU –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
   - –ï—Å–ª–∏ 30-70% ‚Üí –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å
   - –ï—Å–ª–∏ > 70% ‚Üí –æ—Ç–ª–∏—á–Ω–æ!

‚úÖ GPU Memory: 80-95% –∑–∞–Ω—è—Ç–æ
   - –ï—Å–ª–∏ < 60% ‚Üí batch size –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å
   - –ï—Å–ª–∏ 80-95% ‚Üí –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è 16GB
   - –ï—Å–ª–∏ > 95% ‚Üí —Ä–∏—Å–∫ OOM

‚úÖ Temperature: < 85¬∞C
   - –ï—Å–ª–∏ < 75¬∞C ‚Üí –æ—Ç–ª–∏—á–Ω–æ
   - –ï—Å–ª–∏ 75-85¬∞C ‚Üí –Ω–æ—Ä–º–∞–ª—å–Ω–æ
   - –ï—Å–ª–∏ > 85¬∞C ‚Üí –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—Ö–ª–∞–∂–¥–µ–Ω–∏–µ
```

#### 2. Training Loss
```
‚úÖ –î–æ–ª–∂–µ–Ω –°–ù–ò–ñ–ê–¢–¨–°–Ø —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º

–•–æ—Ä–æ—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è:
  –ù–∞—á–∞–ª–æ:  3.0-5.0  (–ø–µ—Ä–≤—ã–µ 100 —à–∞–≥–æ–≤)
  –ß–µ—Ä–µ–∑ —á–∞—Å: 1.5-2.5
  –ß–µ—Ä–µ–∑ 12—á: 0.8-1.5
  –ö–æ–Ω–µ—Ü:   0.3-0.8  (–ø–æ—Å–ª–µ 1-2 –¥–Ω–µ–π)

‚ö†Ô∏è –ü–ª–æ—Ö–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:
  - Loss —Ä–∞—Å—Ç–µ—Ç ‚Üí —É–º–µ–Ω—å—à–∏—Ç–µ learning rate
  - Loss = NaN ‚Üí –æ–±—É—á–µ–Ω–∏–µ —Å–ª–æ–º–∞–ª–æ—Å—å, –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ
  - Loss –∑–∞—Å—Ç—Ä—è–ª (–Ω–µ –º–µ–Ω—è–µ—Ç—Å—è) ‚Üí –ø—Ä–æ–±–ª–µ–º–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ –º–æ–¥–µ–ª—å—é
```

#### 3. Eval Loss
```
‚úÖ –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–ª–∏–∑–∫–æ –∫ Train Loss

–•–æ—Ä–æ—à–æ:
  Train: 0.85  Eval: 0.95  (—Ä–∞–∑–Ω–∏—Ü–∞ < 20%)

‚ö†Ô∏è –ü–ª–æ—Ö–æ (Overfitting):
  Train: 0.40  Eval: 1.20  (eval >> train)
  
–†–µ—à–µ–Ω–∏–µ overfitting:
  - –£–≤–µ–ª–∏—á–∏—Ç—å weight_decay
  - –î–æ–±–∞–≤–∏—Ç—å dropout
  - –£–º–µ–Ω—å—à–∏—Ç—å epochs
```

#### 4. Learning Rate
```
‚úÖ –î–æ–ª–∂–µ–Ω –ø–ª–∞–≤–Ω–æ –∏–∑–º–µ–Ω—è—Ç—å—Å—è

–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:
  –ù–∞—á–∞–ª–æ:  ~2e-4  (warmup)
  –°–µ—Ä–µ–¥–∏–Ω–∞: 2e-4  (stable)
  –ö–æ–Ω–µ—Ü:   ~1e-5  (decay)

–ì—Ä–∞—Ñ–∏–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å:
  Warmup: ‚Üó (—Ä–∞—Å—Ç–µ—Ç)
  Main:   ‚Üí (—Å—Ç–∞–±–∏–ª—å–Ω—ã–π)
  Decay:  ‚Üò (—Å–Ω–∏–∂–∞–µ—Ç—Å—è)
```

#### 5. –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
```
‚úÖ –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –Ω–∞ 16GB GPU:

  ~1-3 sec/iteration (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç batch size)
  ~200-500 iterations/—á–∞—Å
  ~5000-12000 iterations/–¥–µ–Ω—å

–ï—Å–ª–∏ –º–µ–¥–ª–µ–Ω–Ω–µ–µ:
  - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ GPU utilization
  - –£–º–µ–Ω—å—à–∏—Ç–µ MAX_SEQ_LENGTH
  - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ —á—Ç–æ-—Ç–æ –Ω–∞ CPU
```

---

## üõ†Ô∏è –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```bash
# –í –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ
conda activate rpa
cd /home/zarina/Work/RPA/gemma_finetuning
python scripts/monitor_training.py
```

**–ß—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç:**
- üéÆ GPU —Å—Ç–∞—Ç—É—Å (–∑–∞–≥—Ä—É–∑–∫–∞, –ø–∞–º—è—Ç—å, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞)
- üìä –ú–µ—Ç—Ä–∏–∫–∏ (loss, learning rate)
- üè• –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (–ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è)
- üéØ –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ (–≤—Å–µ –û–ö / –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã)

–û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥!

---

### –í–∞—Ä–∏–∞–Ω—Ç 2: TensorBoard (–≤–∏–∑—É–∞–ª—å–Ω—ã–π)

```bash
# –í –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ
conda activate rpa
cd /home/zarina/Work/RPA/gemma_finetuning
tensorboard --logdir=logs/
```

–û—Ç–∫—Ä–æ–π—Ç–µ –≤ –±—Ä–∞—É–∑–µ—Ä–µ: **http://localhost:6006**

**–ß—Ç–æ —Å–º–æ—Ç—Ä–µ—Ç—å:**
- `loss` ‚Üí –¥–æ–ª–∂–µ–Ω —Å–Ω–∏–∂–∞—Ç—å—Å—è ‚Üò
- `eval_loss` ‚Üí –¥–æ–ª–∂–µ–Ω —Å–Ω–∏–∂–∞—Ç—å—Å—è –∏ –±—ã—Ç—å –±–ª–∏–∑–∫–æ –∫ loss
- `learning_rate` ‚Üí –≥—Ä–∞—Ñ–∏–∫ warmup ‚Üí stable ‚Üí decay
- `train/samples_per_second` ‚Üí —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è

---

### –í–∞—Ä–∏–∞–Ω—Ç 3: –†—É—á–Ω–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU

```bash
# –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç GPU –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É
watch -n 1 nvidia-smi
```

**–ß—Ç–æ —Å–º–æ—Ç—Ä–µ—Ç—å:**
- `GPU-Util`: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 70-100%
- `Memory-Usage`: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~14-15 GB –∏–∑ 16 GB
- `Temperature`: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å < 85¬∞C

---

### –í–∞—Ä–∏–∞–Ω—Ç 4: –õ–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è

```bash
# –°–º–æ—Ç—Ä–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –ª–æ–≥–∞
tail -f outputs/gemma_qlora_*/logs.txt
```

**–ß—Ç–æ –∏—Å–∫–∞—Ç—å:**
- `{'loss': ...}` ‚Üí –∑–Ω–∞—á–µ–Ω–∏–µ loss
- `{'eval_loss': ...}` ‚Üí –∑–Ω–∞—á–µ–Ω–∏–µ eval loss
- –û—à–∏–±–∫–∏: `Error`, `Exception`, `CUDA out of memory`

---

## üö® –¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### –ü—Ä–æ–±–ª–µ–º–∞ 1: Out of Memory (OOM)
```
–°–∏–º–ø—Ç–æ–º—ã:
  - CUDA out of memory error
  - –û–±—É—á–µ–Ω–∏–µ –∫—Ä–∞—à–∏—Ç—Å—è
  
–†–µ—à–µ–Ω–∏–µ:
  1. –í train_qlora.py —É–º–µ–Ω—å—à–∏—Ç–µ:
     BATCH_SIZE = 1
     GRADIENT_ACCUMULATION = 16
     MAX_SEQ_LENGTH = 256
  
  2. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ
```

### –ü—Ä–æ–±–ª–µ–º–∞ 2: GPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
```
–°–∏–º–ø—Ç–æ–º—ã:
  - GPU-Util = 0% –∏–ª–∏ –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–π
  - –û–±—É—á–µ–Ω–∏–µ –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ–µ
  
–†–µ—à–µ–Ω–∏–µ:
  1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ CUDA:
     python -c "import torch; print(torch.cuda.is_available())"
  
  2. –ï—Å–ª–∏ False ‚Üí –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch:
     pip install torch --index-url https://download.pytorch.org/whl/cu121
  
  3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ
```

### –ü—Ä–æ–±–ª–µ–º–∞ 3: Loss = NaN
```
–°–∏–º–ø—Ç–æ–º—ã:
  - loss –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç NaN
  - –û–±—É—á–µ–Ω–∏–µ —Å–ª–æ–º–∞–ª–æ—Å—å
  
–†–µ—à–µ–Ω–∏–µ:
  1. –í train_qlora.py —É–º–µ–Ω—å—à–∏—Ç–µ:
     LEARNING_RATE = 1e-4  (–±—ã–ª–æ 2e-4)
  
  2. –ò–ª–∏ —É–≤–µ–ª–∏—á—å—Ç–µ:
     WARMUP_RATIO = 0.1  (–±—ã–ª–æ 0.03)
  
  3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ checkpoint
```

### –ü—Ä–æ–±–ª–µ–º–∞ 4: Loss –∑–∞—Å—Ç—Ä—è–ª (–Ω–µ –º–µ–Ω—è–µ—Ç—Å—è)
```
–°–∏–º–ø—Ç–æ–º—ã:
  - Loss –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è 1000+ steps
  - –ó–Ω–∞—á–µ–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
  
–†–µ—à–µ–Ω–∏–µ:
  1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ learning rate (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π)
  2. –£–≤–µ–ª–∏—á—å—Ç–µ LEARNING_RATE –¥–æ 3e-4
  3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ (–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ)
```

### –ü—Ä–æ–±–ª–µ–º–∞ 5: Overfitting
```
–°–∏–º–ø—Ç–æ–º—ã:
  - Train loss << Eval loss
  - –†–∞–∑–Ω–∏—Ü–∞ –±–æ–ª—å—à–µ 50%
  
–†–µ—à–µ–Ω–∏–µ:
  1. –£–º–µ–Ω—å—à–∏—Ç–µ epochs: NUM_EPOCHS = 2
  2. –£–≤–µ–ª–∏—á—å—Ç–µ WEIGHT_DECAY = 0.01
  3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ early stopping (—É–∂–µ –≤—Å—Ç—Ä–æ–µ–Ω)
```

---

## üìä –û–∂–∏–¥–∞–µ–º—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å (1-2 –¥–Ω—è –æ–±—É—á–µ–Ω–∏—è)

### –ß–∞—Å 1-4 (Warmup + –Ω–∞—á–∞–ª–æ)
```
Loss: 5.0 ‚Üí 2.5
GPU: 85-95%
–ü–∞–º—è—Ç—å: 14-15 GB
–°–∫–æ—Ä–æ—Å—Ç—å: ~2 sec/iter
```
‚úÖ **–í—Å–µ –û–ö –µ—Å–ª–∏:** Loss –±—ã—Å—Ç—Ä–æ –ø–∞–¥–∞–µ—Ç

### –ß–∞—Å 5-12 (–û—Å–Ω–æ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
```
Loss: 2.5 ‚Üí 1.2
Eval Loss: –ø–æ—è–≤–ª—è–µ—Ç—Å—è, –±–ª–∏–∑–∫–æ –∫ Train Loss
–°–∫–æ—Ä–æ—Å—Ç—å: —Å—Ç–∞–±–∏–ª—å–Ω–∞—è
```
‚úÖ **–í—Å–µ –û–ö –µ—Å–ª–∏:** Loss —Å—Ç–∞–±–∏–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è

### –ß–∞—Å 13-24 (Epoch 2)
```
Loss: 1.2 ‚Üí 0.7
Eval Loss: 0.8-1.0
–£–ª—É—á—à–µ–Ω–∏–µ –∑–∞–º–µ–¥–ª—è–µ—Ç—Å—è
```
‚úÖ **–í—Å–µ –û–ö –µ—Å–ª–∏:** Eval loss –Ω–µ —Ä–∞—Å—Ç–µ—Ç

### –ß–∞—Å 25-48 (Epoch 3 + –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ)
```
Loss: 0.7 ‚Üí 0.4-0.6
Eval Loss: 0.6-0.8
Best model —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è
```
‚úÖ **–í—Å–µ –û–ö –µ—Å–ª–∏:** Eval loss —Å—Ç–∞–±–∏–ª–µ–Ω –∏–ª–∏ —Å–Ω–∏–∂–∞–µ—Ç—Å—è

---

## ‚úÖ –ß–µ–∫–ª–∏—Å—Ç "–í—Å–µ —Ö–æ—Ä–æ—à–æ"

–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∫–∞–∂–¥—ã–µ 2-4 —á–∞—Å–∞:

- [ ] GPU Utilization > 70%
- [ ] GPU Memory –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 80-95%
- [ ] Temperature < 85¬∞C
- [ ] Train Loss —Å–Ω–∏–∂–∞–µ—Ç—Å—è
- [ ] Eval Loss –±–ª–∏–∑–∫–æ –∫ Train Loss (—Ä–∞–∑–Ω–∏—Ü–∞ < 30%)
- [ ] Learning Rate —Å–ª–µ–¥—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫—É (warmup ‚Üí stable ‚Üí decay)
- [ ] –°–∫–æ—Ä–æ—Å—Ç—å ~1-3 sec/iteration
- [ ] –ù–µ—Ç –æ—à–∏–±–æ–∫ –≤ –ª–æ–≥–∞—Ö
- [ ] Checkpoint'—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è (–∫–∞–∂–¥—ã–µ 500 steps)

**–ï—Å–ª–∏ –≤—Å–µ ‚úÖ ‚Üí —Ä–∞—Å—Å–ª–∞–±—å—Ç–µ—Å—å, –≤—Å–µ –∏–¥–µ—Ç –æ—Ç–ª–∏—á–Ω–æ!** üéâ

**–ï—Å–ª–∏ –µ—Å—Ç—å ‚ùå ‚Üí —Å–º. —Ä–∞–∑–¥–µ–ª "–¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã"** ‚¨ÜÔ∏è

---

## üéØ –ö–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ?

### ‚úÖ –•–æ—Ä–æ—à–∏–µ –ø—Ä–∏—á–∏–Ω—ã –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å:

1. **–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å** (3 epochs)
2. **Early stopping —Å—Ä–∞–±–æ—Ç–∞–ª** (eval loss –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è)
3. **–î–æ—Å—Ç–∏–≥–ª–∏ —Ü–µ–ª–µ–≤–æ–π accuracy** (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ —á–µ—Ä–µ–∑ evaluate.py)

### ‚ö†Ô∏è –ü–ª–æ—Ö–∏–µ –ø—Ä–∏—á–∏–Ω—ã –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å:

1. "Loss –Ω–µ –ø–∞–¥–∞–µ—Ç —É–∂–µ 30 –º–∏–Ω—É—Ç"
   ‚Üí –ü–æ–¥–æ–∂–¥–∏—Ç–µ –º–∏–Ω–∏–º—É–º 2-4 —á–∞—Å–∞

2. "Eval loss —Ö—É–∂–µ train loss"
   ‚Üí –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ! –†–∞–∑–Ω–∏—Ü–∞ –¥–æ 30% OK

3. "–°–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ"
   ‚Üí Fine-tuning BERT –∑–∞–Ω–∏–º–∞–µ—Ç —á–∞—Å—ã, Gemma - –¥–Ω–∏

### üõë –ö–æ–≥–¥–∞ –ù–£–ñ–ù–û –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å:

1. Loss = NaN (—Å–ª–æ–º–∞–ª–æ—Å—å)
2. CUDA Out of Memory (–Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏)
3. GPU –ø–µ—Ä–µ–≥—Ä–µ–≤–∞–µ—Ç—Å—è (> 90¬∞C)
4. –Ø–≤–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ –ª–æ–≥–∞—Ö

---

## üí° Pro Tips

1. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ TensorBoard —Å—Ä–∞–∑—É**
   ```bash
   tensorboard --logdir=logs/
   ```
   –û—Å—Ç–∞–≤—å—Ç–µ –æ—Ç–∫—Ä—ã—Ç—ã–º, —Å–º–æ—Ç—Ä–∏—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏

2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ tmux/screen**
   ```bash
   tmux new -s training
   python scripts/train_qlora.py
   # Ctrl+B, –ø–æ—Ç–æ–º D –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è
   # tmux attach -t training –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
   ```
   –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—Å—è –¥–∞–∂–µ –µ—Å–ª–∏ –∑–∞–∫—Ä–æ–µ—Ç–µ —Ç–µ—Ä–º–∏–Ω–∞–ª

3. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ**
   ```bash
   python scripts/monitor_training.py
   ```
   –í–∏–¥–∏—Ç–µ –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ

4. **–°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –ª–æ–≥–∏ GPU**
   ```bash
   nvidia-smi dmon -s pucvmet > gpu_log.txt &
   ```
   –ü–æ–ª–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU

---

## üé¨ –ì–æ—Ç–æ–≤—ã –∫ –∑–∞–ø—É—Å–∫—É?

1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ:
   ```bash
   python scripts/train_qlora.py
   ```

2. –í –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:
   ```bash
   python scripts/monitor_training.py
   ```

3. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ - TensorBoard:
   ```bash
   tensorboard --logdir=logs/
   ```

4. –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Å—Ç–∞—Ç—É—Å –∫–∞–∂–¥—ã–µ 2-4 —á–∞—Å–∞

5. –ñ–¥–∏—Ç–µ 1-2 –¥–Ω—è ‚è∞

6. Profit! üéâ

---

**–£–¥–∞—á–∏ —Å –æ–±—É—á–µ–Ω–∏–µ–º! –í—Å–µ –±—É–¥–µ—Ç —Ö–æ—Ä–æ—à–æ! üí™**


# üìã –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞ QLoRA Fine-tuning Gemma 3:12b

## ‚úÖ –ß—Ç–æ —Å–æ–∑–¥–∞–Ω–æ

–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è fine-tuning Gemma 3:12b –Ω–∞ –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –ø–ª–∞—Ç–µ–∂–µ–π.

### üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
gemma_finetuning/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep                    # (–ø–æ—Å–ª–µ prepare_data.py –ø–æ—è–≤—è—Ç—Å—è train/val/test)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models/                      # –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã)
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep                    # (–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –ø–æ—è–≤–∏—Ç—Å—è gemma_qlora_*)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ outputs/                     # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã evaluation
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep                    # (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ logs/                        # TensorBoard –ª–æ–≥–∏
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep                    # (–ª–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/                     # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ prepare_data.py             # ‚≠ê –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îú‚îÄ‚îÄ train_qlora.py              # ‚≠ê QLoRA fine-tuning
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py                 # ‚≠ê –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ inference.py                # ‚≠ê –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îî‚îÄ‚îÄ check_environment.py        # üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                # üì¶ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ README.md                       # üìñ –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îú‚îÄ‚îÄ PROJECT_OVERVIEW.md             # üìã –≠—Ç–æ—Ç —Ñ–∞–π–ª
‚îú‚îÄ‚îÄ quickstart.sh                   # üöÄ –°–∫—Ä–∏–ø—Ç –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
‚îî‚îÄ‚îÄ .gitignore                      # Git ignore –ø—Ä–∞–≤–∏–ª–∞
```

## üéØ –ó–∞–¥–∞—á–∞

**–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:** `PaymentComment ‚Üí OperCode`

- **–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:** 388,706 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∏–∑ `final_dataset.csv`
- **–ö–ª–∞—Å—Å–æ–≤:** 86 (OperCode)
- **–Ø–∑—ã–∫–∏:** –†—É—Å—Å–∫–∏–π + –ê–Ω–≥–ª–∏–π—Å–∫–∏–π
- **–ú–µ—Ç–æ–¥:** QLoRA Fine-tuning Gemma 3:12b

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```bash
# –ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ conda –æ–∫—Ä—É–∂–µ–Ω–∏–µ
conda activate rpa

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements.txt

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Å—ë –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π
./quickstart.sh
```

### –í–∞—Ä–∏–∞–Ω—Ç 2: –ü–æ—à–∞–≥–æ–≤—ã–π

```bash
# –ê–∫—Ç–∏–≤–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è
conda activate rpa

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt

# –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞
python scripts/check_environment.py

# –®–∞–≥ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
python scripts/prepare_data.py

# –®–∞–≥ 3: –û–±—É—á–µ–Ω–∏–µ (1-2 –¥–Ω—è)
python scripts/train_qlora.py

# –®–∞–≥ 4: –û—Ü–µ–Ω–∫–∞
python scripts/evaluate.py

# –®–∞–≥ 5: Inference
python scripts/inference.py
```

## üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è 16GB VRAM

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤ `train_qlora.py`:

```python
# QLoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
LORA_R = 16                    # –†–∞–Ω–≥ LoRA
LORA_ALPHA = 32                # Alpha
USE_4BIT = True                # 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è

# Batch –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
BATCH_SIZE = 2                 # –ú–∞–ª–µ–Ω—å–∫–∏–π batch
GRADIENT_ACCUMULATION = 8      # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch = 16

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
GRADIENT_CHECKPOINTING = True  # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
OPTIM = "paged_adamw_8bit"     # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
MAX_SEQ_LENGTH = 512           # –ú–∞–∫—Å. –¥–ª–∏–Ω–∞
```

**–û–∂–∏–¥–∞–µ–º–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏:** ~14-15 GB (–≤–ª–µ–∑–µ—Ç –≤ 16GB!)

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ |
|---------|----------|
| **Accuracy** | 90-95% |
| **F1-weighted** | 89-94% |
| **F1-macro** | 75-85% |
| **Training time** | 1-2 –¥–Ω—è |
| **Inference** | ~1-2 —Å–µ–∫/–ø—Ä–∏–º–µ—Ä |
| **Model size** | ~200-500 MB (—Ç–æ–ª—å–∫–æ LoRA) |

## üìù –û—Å–Ω–æ–≤–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã

### 1Ô∏è‚É£ prepare_data.py
**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç `final_dataset.csv`
- –°–æ–∑–¥–∞–µ—Ç stratified split (70/15/15)
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è Gemma
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ CSV –∏ JSONL —Ñ–æ—Ä–º–∞—Ç–∞—Ö

**–í—ã—Ö–æ–¥:**
- `data/train.jsonl` (~272k –ø—Ä–∏–º–µ—Ä–æ–≤)
- `data/val.jsonl` (~58k –ø—Ä–∏–º–µ—Ä–æ–≤)  
- `data/test.jsonl` (~58k –ø—Ä–∏–º–µ—Ä–æ–≤)
- `data/metadata.json`

### 2Ô∏è‚É£ train_qlora.py
**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç Gemma 2 9B (–∏–ª–∏ Gemma 3:12b)
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é
- –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
- –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–¥–∞–ø—Ç–µ—Ä—ã + –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é

**–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**
```bash
# –í –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ
tensorboard --logdir=logs/
```

**–í—ã—Ö–æ–¥:**
- `models/gemma_qlora_TIMESTAMP/` (LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã ~200-500MB)

### 3Ô∏è‚É£ evaluate.py
**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
- –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –Ω–∞ test set
- –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

**–ú–µ—Ç—Ä–∏–∫–∏:**
- Accuracy
- F1-score (weighted, macro)
- Precision, Recall
- Classification report

### 4Ô∏è‚É£ inference.py
**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å
- –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã:
  - –î–µ–º–æ —Ä–µ–∂–∏–º (–ø—Ä–∏–º–µ—Ä—ã)
  - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
  - API –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–æ–¥–µ

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
from inference import OperCodeClassifier

classifier = OperCodeClassifier("models/gemma_qlora_TIMESTAMP")
opercode = classifier.predict("TRANSFER OF FUNDS TO OWN ACCOUNT")
print(f"OperCode: {opercode}")
```

## üéì –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏

### –ü—Ä–æ–º–ø—Ç —Ñ–æ—Ä–º–∞—Ç
```
<start_of_turn>user
–û–ø—Ä–µ–¥–µ–ª–∏ –∫–æ–¥ –æ–ø–µ—Ä–∞—Ü–∏–∏ (OperCode) –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ –ø–ª–∞—Ç–µ–∂–∞:

–ü–ª–∞—Ç—ë–∂: [—Ç–µ–∫—Å—Ç –ø–ª–∞—Ç–µ–∂–∞]

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–¥–æ–º –æ–ø–µ—Ä–∞—Ü–∏–∏.<end_of_turn>
<start_of_turn>model
[OperCode]<end_of_turn>
```

### QLoRA (Quantized LoRA)
- **–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å:** –ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–∞ –≤ 4-bit (NF4)
- **LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã:** –û–±—É—á–∞—é—Ç—Å—è –≤ bfloat16
- **–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:** ~0.5-1% –æ—Ç –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **–≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏:** ~4x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å full fine-tuning

### –°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è
1. Stratified split (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤)
2. Class weights (–¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞)
3. Gradient accumulation (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –±–æ–ª—å—à–æ–π batch)
4. Warmup + cosine schedule
5. Early stopping (best model —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è)

## üêõ Troubleshooting

### Out of Memory
```python
# –í train_qlora.py –∏–∑–º–µ–Ω–∏—Ç–µ:
BATCH_SIZE = 1
GRADIENT_ACCUMULATION = 16
MAX_SEQ_LENGTH = 256
```

### –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: `nvidia-smi`
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ flash-attention (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
3. –£–º–µ–Ω—å—à–∏—Ç–µ MAX_SEQ_LENGTH –¥–æ 256

### –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
1. –£–≤–µ–ª–∏—á—å—Ç–µ NUM_EPOCHS: 3 ‚Üí 5
2. –£–≤–µ–ª–∏—á—å—Ç–µ LORA_R: 16 ‚Üí 32
3. –î–æ–±–∞–≤—å—Ç–µ data augmentation –¥–ª—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤

## üìà –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

### –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:
1. ‚úÖ Evaluate –Ω–∞ test set
2. ‚úÖ –ê–Ω–∞–ª–∏–∑ confusion matrix
3. ‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ inference
4. üîÑ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)

### –ü—Ä–æ–¥–∞–∫—à–Ω deployment:
1. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ ONNX (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
2. –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è inference (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
3. API –æ–±–µ—Ä—Ç–∫–∞ (FastAPI)
4. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞

## üí° –°–æ–≤–µ—Ç—ã –ø–æ —É–ª—É—á—à–µ–Ω–∏—é

### –ï—Å–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å < 90%:
- –£–≤–µ–ª–∏—á—å—Ç–µ epochs (3 ‚Üí 5)
- –£–≤–µ–ª–∏—á—å—Ç–µ LoRA rank (16 ‚Üí 32)
- –î–æ–±–∞–≤—å—Ç–µ augmentation
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ class weights

### –ï—Å–ª–∏ –¥–æ–ª–≥–æ –æ–±—É—á–∞–µ—Ç—Å—è:
- –£–º–µ–Ω—å—à–∏—Ç–µ MAX_SEQ_LENGTH
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–≤—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ flash-attention-2

### –ï—Å–ª–∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:
- –£–º–µ–Ω—å—à–∏—Ç–µ learning rate (2e-4 ‚Üí 1e-4)
- –£–≤–µ–ª–∏—á—å—Ç–µ warmup_ratio (0.03 ‚Üí 0.1)
- –î–æ–±–∞–≤—å—Ç–µ weight_decay

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [Gemma Documentation](https://ai.google.dev/gemma)
- [PEFT Library](https://github.com/huggingface/peft)
- [Transformers Docs](https://huggingface.co/docs/transformers)

## ‚úÖ Checklist –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º

- [ ] Conda –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–æ (`conda activate rpa`)
- [ ] –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã (`pip install -r requirements.txt`)
- [ ] PyTorch —Å CUDA —Ä–∞–±–æ—Ç–∞–µ—Ç (`python -c "import torch; print(torch.cuda.is_available())"`)
- [ ] GPU –¥–æ—Å—Ç—É–ø–Ω–∞ (16GB VRAM –º–∏–Ω–∏–º—É–º)
- [ ] `final_dataset.csv` –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
- [ ] –°–≤–æ–±–æ–¥–Ω–æ ~30GB –Ω–∞ –¥–∏—Å–∫–µ
- [ ] –ó–∞–ø—É—â–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è (`python scripts/check_environment.py`)

## üéØ –ì–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É!

–í—Å—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –∏ –≥–æ—Ç–æ–≤–æ. –ü—Ä–æ—Å—Ç–æ —Å–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ **–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç** –≤—ã—à–µ.

**–£–¥–∞—á–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏! üöÄ**

---

**–ü—Ä–æ–µ–∫—Ç:** RPA SWIFT Transaction Classification  
**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 2025-11-04  
**–ê–≤—Ç–æ—Ä:** AI Assistant  
**–î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤:** –°–º. README.md


═══════════════════════════════════════════════════════════════════════════════
                    🔍 БЫСТРАЯ ПРОВЕРКА "ВСЁ ЛИ ХОРОШО?"
═══════════════════════════════════════════════════════════════════════════════

🚀 ТРИ СПОСОБА МОНИТОРИНГА:

1️⃣  АВТОМАТИЧЕСКИЙ (ЛУЧШИЙ!)
    python scripts/monitor_training.py
    → Показывает все: GPU, метрики, проблемы, советы
    → Обновляется каждые 10 сек
    
2️⃣  TENSORBOARD (ВИЗУАЛЬНЫЙ)
    tensorboard --logdir=logs/
    → Откройте http://localhost:6006
    → Графики loss, learning rate и т.д.
    
3️⃣  РУЧНОЙ (БАЗОВЫЙ)
    watch -n 1 nvidia-smi
    → GPU загрузка, память, температура

═══════════════════════════════════════════════════════════════════════════════
✅ ВСЁ ХОРОШО, ЕСЛИ:
═══════════════════════════════════════════════════════════════════════════════

[ ] GPU загрузка 70-100%         (nvidia-smi → GPU-Util)
[ ] GPU память 14-15 GB из 16    (nvidia-smi → Memory-Usage)
[ ] Температура < 85°C           (nvidia-smi → Temp)
[ ] Loss снижается со временем   (TensorBoard → loss график ↘)
[ ] Нет ошибок в выводе          (смотрите терминал)

Если все ✅ → ВСЁ ОТЛИЧНО! Ждите завершения (1-2 дня)

═══════════════════════════════════════════════════════════════════════════════
❌ ПРОБЛЕМЫ И БЫСТРЫЕ РЕШЕНИЯ:
═══════════════════════════════════════════════════════════════════════════════

ПРОБЛЕМА: "CUDA out of memory"
┌─────────────────────────────────────────────────────────────────────────┐
│ РЕШЕНИЕ:                                                                │
│ 1. Откройте scripts/train_qlora.py                                     │
│ 2. Измените: BATCH_SIZE = 1                                            │
│ 3. Измените: MAX_SEQ_LENGTH = 256                                      │
│ 4. Сохраните и перезапустите обучение                                  │
└─────────────────────────────────────────────────────────────────────────┘

ПРОБЛЕМА: GPU не используется (GPU-Util = 0%)
┌─────────────────────────────────────────────────────────────────────────┐
│ РЕШЕНИЕ:                                                                │
│ 1. Проверьте: python -c "import torch; print(torch.cuda.is_available())"│
│ 2. Если False → установите PyTorch с CUDA:                             │
│    pip install torch --index-url https://download.pytorch.org/whl/cu121│
│ 3. Перезапустите обучение                                              │
└─────────────────────────────────────────────────────────────────────────┘

ПРОБЛЕМА: Loss = NaN или не меняется
┌─────────────────────────────────────────────────────────────────────────┐
│ РЕШЕНИЕ:                                                                │
│ 1. Откройте scripts/train_qlora.py                                     │
│ 2. Измените: LEARNING_RATE = 1e-4  (было 2e-4)                        │
│ 3. Сохраните и перезапустите                                           │
└─────────────────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════
📊 ОЖИДАЕМЫЕ ЗНАЧЕНИЯ (нормальное обучение):
═══════════════════════════════════════════════════════════════════════════

GPU Utilization:     80-95%      ✅
GPU Memory Used:     14-15 GB    ✅ (из 16 GB)
Temperature:         65-80°C     ✅
Speed:               1-3 sec/it  ✅

Loss (начало):       3.0-5.0     🔴
Loss (через час):    1.5-2.5     🟡
Loss (через 12ч):    0.8-1.5     🟢
Loss (конец 1-2дн):  0.4-0.8     ✅

Eval Loss:           близко к Train Loss (разница < 30%)

═══════════════════════════════════════════════════════════════════════════
⏱️  ВРЕМЕННАЯ ШКАЛА (что ожидать):
═══════════════════════════════════════════════════════════════════════════

⏰ Первые 30 минут:
   • Loss быстро падает (5.0 → 3.0)
   • GPU разогревается
   • Первые checkpoint'ы сохраняются
   ✅ Нормально

⏰ Первые 4 часа:
   • Loss продолжает падать (3.0 → 1.5)
   • Появляется eval_loss
   • Скорость стабилизируется
   ✅ Нормально

⏰ Через 12 часов:
   • Loss ~1.0
   • Eval loss близок к train loss
   • Идет второй epoch
   ✅ Нормально

⏰ Через 24-48 часов:
   • Loss ~0.5-0.7
   • Best model сохраняется
   • Скоро завершится!
   ✅ Отлично!

═══════════════════════════════════════════════════════════════════════════
🆘 НУЖНА ПОМОЩЬ?
═══════════════════════════════════════════════════════════════════════════

Полное руководство:     MONITORING_GUIDE.md
Документация:           README.md
Обзор проекта:          PROJECT_OVERVIEW.md

Команды для диагностики:
  • nvidia-smi                              → GPU статус
  • python scripts/monitor_training.py      → Автоматический мониторинг
  • tensorboard --logdir=logs/              → TensorBoard
  • tail -f outputs/gemma_qlora_*/logs.txt  → Логи обучения

═══════════════════════════════════════════════════════════════════════════

📝 ЗАМЕТКИ:

• Обучение занимает 1-2 дня - это нормально!
• Loss не будет падать постоянно - могут быть плато
• Небольшие колебания loss - это нормально
• Eval loss может быть чуть выше train loss - это OK
• Если GPU работает и loss снижается → всё идет хорошо! ✅

═══════════════════════════════════════════════════════════════════════════
                        🎉 УДАЧИ В ОБУЧЕНИИ! 🚀
═══════════════════════════════════════════════════════════════════════════


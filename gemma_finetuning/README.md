# üöÄ QLoRA Fine-tuning Gemma 3:12b –¥–ª—è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ë–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –ü–ª–∞—Ç–µ–∂–µ–π

Fine-tuning Gemma 3:12b —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º QLoRA –¥–ª—è –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ `PaymentComment ‚Üí OperCode`.

## üìä –ó–∞–¥–∞—á–∞

**–í—Ö–æ–¥:** –¢–µ–∫—Å—Ç –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ –ø–ª–∞—Ç–µ–∂–∞ (PaymentComment)  
**–í—ã—Ö–æ–¥:** –ö–æ–¥ –æ–ø–µ—Ä–∞—Ü–∏–∏ (OperCode) - 86 –∫–ª–∞—Å—Å–æ–≤

**–î–∞–Ω–Ω—ã–µ:** 388,706 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∏–∑ SWIFT —Å–∏—Å—Ç–µ–º–∞

## üéØ –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- ‚úÖ **QLoRA** - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ fine-tuning –¥–ª—è 16GB VRAM
- ‚úÖ **4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è** - —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
- ‚úÖ **Gradient checkpointing** - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
- ‚úÖ **Stratified split** - —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ **–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å** - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ

## üíª –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ñ–µ–ª–µ–∑–æ
- **GPU:** 16GB VRAM (RTX 3060 Ti, 4060 Ti, 3090, 4090)
- **RAM:** 32GB —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
- **–î–∏—Å–∫:** ~50GB —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞

### –°–æ—Ñ—Ç
- Python 3.10+
- CUDA 11.8+ –∏–ª–∏ 12.1+
- Linux (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è) –∏–ª–∏ WSL2

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### 1. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–µ—Ä–µ—Ö–æ–¥ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
```bash
cd /home/zarina/Work/RPA/gemma_finetuning
```

### 2. –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# –∏–ª–∏
venv\Scripts\activate  # Windows
```

### 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –°–Ω–∞—á–∞–ª–∞ PyTorch —Å CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# –ó–∞—Ç–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements.txt
```

**–ü—Ä–æ–≤–µ—Ä–∫–∞ GPU:**
```bash
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')"
```

## üèÉ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

```bash
cd scripts
python prepare_data.py
```

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç `final_dataset.csv`
- –°–æ–∑–¥–∞–µ—Ç train/val/test split (70/15/15)
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è Gemma
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ `data/` –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV –∏ JSONL

**–í—ã—Ö–æ–¥:**
```
data/
‚îú‚îÄ‚îÄ train.csv & train.jsonl  (~272k –ø—Ä–∏–º–µ—Ä–æ–≤)
‚îú‚îÄ‚îÄ val.csv & val.jsonl      (~58k –ø—Ä–∏–º–µ—Ä–æ–≤)
‚îú‚îÄ‚îÄ test.csv & test.jsonl    (~58k –ø—Ä–∏–º–µ—Ä–æ–≤)
‚îî‚îÄ‚îÄ metadata.json
```

### –®–∞–≥ 2: Fine-tuning

```bash
python train_qlora.py
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –≤ —Å–∫—Ä–∏–ø—Ç–µ):**
- Batch size: 2 x 8 accumulation = 16 —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π
- Learning rate: 2e-4
- Epochs: 3
- LoRA r=16, alpha=32

**–í—Ä–µ–º—è:** ~1-2 –¥–Ω—è –Ω–∞ RTX 3060 Ti

**–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏:**
```bash
# –í –¥—Ä—É–≥–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ
tensorboard --logdir=logs/
```
–û—Ç–∫—Ä—ã—Ç—å: http://localhost:6006

**–ß—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è:**
```
models/gemma_qlora_YYYYMMDD_HHMMSS/
‚îú‚îÄ‚îÄ adapter_config.json
‚îú‚îÄ‚îÄ adapter_model.safetensors  (~200-500MB)
‚îú‚îÄ‚îÄ training_config.json
‚îî‚îÄ‚îÄ tokenizer/
```

### –®–∞–≥ 3: Evaluation

```bash
python evaluate.py
```

**–ú–µ—Ç—Ä–∏–∫–∏:**
- Accuracy
- F1-score (weighted & macro)
- Precision & Recall
- Classification report

**–í—ã—Ö–æ–¥:**
```
outputs/evaluation_gemma_qlora_YYYYMMDD_HHMMSS/
‚îú‚îÄ‚îÄ evaluation_results.json
‚îî‚îÄ‚îÄ predictions.csv
```

### –®–∞–≥ 4: Inference

```bash
python inference.py
```

**–†–µ–∂–∏–º—ã:**
1. **–î–µ–º–æ** - –ø—Ä–∏–º–µ—Ä—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
2. **–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π** - –≤–≤–æ–¥ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã
3. **–û–±–∞**

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–æ–¥–µ:**
```python
from inference import OperCodeClassifier

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
classifier = OperCodeClassifier("models/gemma_qlora_YYYYMMDD_HHMMSS")

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
payment = "TRANSFER OF FUNDS TO OWN ACCOUNT"
opercode = classifier.predict(payment)
print(f"OperCode: {opercode}")

# –° —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
opercode, confidence = classifier.predict(payment, return_confidence=True)
print(f"OperCode: {opercode} (confidence: {confidence:.2%})")

# Batch
payments = ["...", "...", "..."]
opcodes = classifier.predict_batch(payments)
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥ –≤–∞—à—É GPU

### –ï—Å–ª–∏ 12GB VRAM
–í `train_qlora.py` –∏–∑–º–µ–Ω–∏—Ç–µ:
```python
BATCH_SIZE = 1
GRADIENT_ACCUMULATION = 16
MAX_SEQ_LENGTH = 384
```

### –ï—Å–ª–∏ 24GB+ VRAM
```python
BATCH_SIZE = 4
GRADIENT_ACCUMULATION = 4
LORA_R = 32
LORA_ALPHA = 64
```

### –ï—Å–ª–∏ –Ω–µ—Ç GPU - Google Colab
1. Colab Pro ($10/–º–µ—Å—è—Ü) - RTX A100 40GB
2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –ø—Ä–æ–µ–∫—Ç –≤ Colab
3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

| –ú–µ—Ç—Ä–∏–∫–∞ | –û–∂–∏–¥–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ |
|---------|-------------------|
| Accuracy | 90-95% |
| F1-score (weighted) | 89-94% |
| F1-score (macro) | 75-85% |
| Training time | 1-2 –¥–Ω—è (16GB GPU) |
| Inference time | ~1-2 —Å–µ–∫/–ø—Ä–∏–º–µ—Ä |

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** Macro F1 –Ω–∏–∂–µ –∏–∑-–∑–∞ —Å–∏–ª—å–Ω–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤.

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
gemma_finetuning/
‚îú‚îÄ‚îÄ data/                  # –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ train.csv/jsonl
‚îÇ   ‚îú‚îÄ‚îÄ val.csv/jsonl
‚îÇ   ‚îú‚îÄ‚îÄ test.csv/jsonl
‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
‚îú‚îÄ‚îÄ models/                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã)
‚îÇ   ‚îî‚îÄ‚îÄ gemma_qlora_*/
‚îú‚îÄ‚îÄ outputs/               # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã evaluation
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_*/
‚îú‚îÄ‚îÄ logs/                  # TensorBoard –ª–æ–≥–∏
‚îÇ   ‚îî‚îÄ‚îÄ gemma_qlora_*/
‚îú‚îÄ‚îÄ scripts/               # –°–∫—Ä–∏–ø—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ prepare_data.py    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îú‚îÄ‚îÄ train_qlora.py     # –û–±—É—á–µ–Ω–∏–µ
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py        # –û—Ü–µ–Ω–∫–∞
‚îÇ   ‚îî‚îÄ‚îÄ inference.py       # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ requirements.txt       # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îî‚îÄ‚îÄ README.md             # –≠—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
```

## üêõ Troubleshooting

### Out of Memory (OOM)
```python
# –í train_qlora.py:
BATCH_SIZE = 1
GRADIENT_ACCUMULATION = 16
MAX_SEQ_LENGTH = 256
```

### –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: `watch -n 1 nvidia-smi`
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ flash-attention: `pip install flash-attn --no-build-isolation`
3. –£–º–µ–Ω—å—à–∏—Ç–µ `MAX_SEQ_LENGTH`

### –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π PyTorch –¥–ª—è –≤–∞—à–µ–π CUDA
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø –∫ HuggingFace: `huggingface-cli login`

### –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
1. –£–≤–µ–ª–∏—á—å—Ç–µ `NUM_EPOCHS` (3 ‚Üí 5)
2. –£–≤–µ–ª–∏—á—å—Ç–µ `LORA_R` (16 ‚Üí 32)
3. –£–º–µ–Ω—å—à–∏—Ç–µ `LEARNING_RATE` (2e-4 ‚Üí 1e-4)
4. –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤

## üéì –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

- [QLoRA paper](https://arxiv.org/abs/2305.14314)
- [Gemma documentation](https://ai.google.dev/gemma)
- [PEFT documentation](https://huggingface.co/docs/peft)
- [Transformers documentation](https://huggingface.co/docs/transformers)

## üìß –ö–æ–Ω—Ç–∞–∫—Ç—ã

–ü—Ä–æ–µ–∫—Ç: RPA SWIFT Transaction Classification  
–î–∞—Ç–∞: 2025-11-04

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ª–∏—Ü–µ–Ω–∑–∏–µ–π Gemma –∏ –≤–∞—à–∏–º–∏ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏.

---

**–£–¥–∞—á–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏! üöÄ**


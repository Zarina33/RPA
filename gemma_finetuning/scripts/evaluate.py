"""
Evaluation —Å–∫—Ä–∏–ø—Ç –¥–ª—è fine-tuned –º–æ–¥–µ–ª–∏
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ test set —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
"""

import torch
import json
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from sklearn.metrics import (
    accuracy_score, 
    f1_score, 
    precision_score, 
    recall_score,
    classification_report,
    confusion_matrix
)
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# –ü—É—Ç–∏
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
MODEL_DIR = BASE_DIR / "models"
OUTPUT_DIR = BASE_DIR / "outputs"

def load_model(model_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ fine-tuned –º–æ–¥–µ–ª–∏"""
    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open(model_path / 'training_config.json', 'r') as f:
        config = json.load(f)
    
    base_model_name = config['model_name']
    
    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # Base model
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
    model = PeftModel.from_pretrained(base_model, model_path)
    model = model.merge_and_unload()  # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è inference
    
    print("   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    return model, tokenizer, config

def load_test_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    test_df = pd.read_csv(DATA_DIR / 'test.csv')
    print(f"   Test samples: {len(test_df):,}")
    
    return test_df

def predict(model, tokenizer, text, max_new_tokens=10):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,  # Greedy decoding
            temperature=0.1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å (–ø–æ—Å–ª–µ –ø—Ä–æ–º–ø—Ç–∞)
    prompt_length = len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))
    prediction = generated[prompt_length:].strip()
    
    return prediction

def extract_opercode(prediction_text):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ OperCode –∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —á–∏—Å–ª–æ –≤ –Ω–∞—á–∞–ª–µ –æ—Ç–≤–µ—Ç–∞
    import re
    match = re.search(r'\d+', prediction_text)
    if match:
        return int(match.group())
    return None

def evaluate_model(model, tokenizer, test_df):
    """–ü–æ–ª–Ω–∞—è evaluation –º–æ–¥–µ–ª–∏"""
    print("\nüîç Evaluation –º–æ–¥–µ–ª–∏ –Ω–∞ test set...")
    
    predictions = []
    true_labels = []
    
    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Predicting"):
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è inference
        prompt = f"""<start_of_turn>user
–û–ø—Ä–µ–¥–µ–ª–∏ –∫–æ–¥ –æ–ø–µ—Ä–∞—Ü–∏–∏ (OperCode) –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ –ø–ª–∞—Ç–µ–∂–∞:

–ü–ª–∞—Ç—ë–∂: {row['PaymentComment']}

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–¥–æ–º –æ–ø–µ—Ä–∞—Ü–∏–∏.<end_of_turn>
<start_of_turn>model
"""
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        pred_text = predict(model, tokenizer, prompt)
        pred_code = extract_opercode(pred_text)
        
        predictions.append(pred_code)
        true_labels.append(row['OperCode'])
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy arrays
    predictions = np.array(predictions)
    true_labels = np.array(true_labels)
    
    # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    valid_mask = predictions != None
    valid_predictions = predictions[valid_mask]
    valid_labels = true_labels[valid_mask]
    
    print(f"\n   –í–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {valid_mask.sum()} / {len(predictions)} ({valid_mask.sum()/len(predictions)*100:.1f}%)")
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    results = {
        'accuracy': accuracy_score(valid_labels, valid_predictions),
        'f1_weighted': f1_score(valid_labels, valid_predictions, average='weighted', zero_division=0),
        'f1_macro': f1_score(valid_labels, valid_predictions, average='macro', zero_division=0),
        'precision_weighted': precision_score(valid_labels, valid_predictions, average='weighted', zero_division=0),
        'recall_weighted': recall_score(valid_labels, valid_predictions, average='weighted', zero_division=0),
        'valid_predictions_ratio': valid_mask.sum() / len(predictions),
    }
    
    return results, predictions, true_labels

def print_results(results):
    """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    print("\n" + "=" * 80)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ EVALUATION")
    print("=" * 80)
    
    print(f"\n‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    print(f"   Accuracy:           {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
    print(f"   F1-score (weighted): {results['f1_weighted']:.4f}")
    print(f"   F1-score (macro):    {results['f1_macro']:.4f}")
    print(f"   Precision:          {results['precision_weighted']:.4f}")
    print(f"   Recall:             {results['recall_weighted']:.4f}")
    print(f"   Valid predictions:  {results['valid_predictions_ratio']:.4f} ({results['valid_predictions_ratio']*100:.2f}%)")

def save_results(results, predictions, true_labels, output_path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    with open(output_path / 'evaluation_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    results_df = pd.DataFrame({
        'true_label': true_labels,
        'prediction': predictions,
        'correct': predictions == true_labels
    })
    results_df.to_csv(output_path / 'predictions.csv', index=False)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")

def main():
    print("=" * 80)
    print("üéØ EVALUATION FINE-TUNED –ú–û–î–ï–õ–ò")
    print("=" * 80)
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω—é—é –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    model_dirs = sorted(MODEL_DIR.glob("gemma_qlora_*"))
    if not model_dirs:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π!")
        print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {MODEL_DIR}")
        return
    
    latest_model = model_dirs[-1]
    print(f"\nüìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: {latest_model.name}")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model, tokenizer, config = load_model(latest_model)
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    test_df = load_test_data()
    
    # 3. Evaluation (–Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–ª—è —Ç–µ—Å—Ç–∞, —É–±–µ—Ä–∏—Ç–µ [:100] –¥–ª—è –ø–æ–ª–Ω–æ–π –æ—Ü–µ–Ω–∫–∏)
    print("\n‚ö†Ô∏è  –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è evaluation –Ω–∞ –ø–µ—Ä–≤—ã—Ö 100 –ø—Ä–∏–º–µ—Ä–∞—Ö (–¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞)")
    print("   –î–ª—è –ø–æ–ª–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —É–±–µ—Ä–∏—Ç–µ —Å—Ä–µ–∑ [:100] –≤ –∫–æ–¥–µ")
    results, predictions, true_labels = evaluate_model(model, tokenizer, test_df[:100])
    
    # 4. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print_results(results)
    
    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    output_path = OUTPUT_DIR / f"evaluation_{latest_model.name}"
    save_results(results, predictions, true_labels, output_path)
    
    print("\n" + "=" * 80)
    print("‚úÖ EVALUATION –ó–ê–í–ï–†–®–ï–ù!")
    print("=" * 80)

if __name__ == "__main__":
    main()


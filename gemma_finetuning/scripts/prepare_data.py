"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fine-tuning Gemma 3:12b
–ó–∞–¥–∞—á–∞: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è PaymentComment ‚Üí OperCode
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import json
from pathlib import Path
from collections import Counter

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
RANDOM_SEED = 42
TEST_SIZE = 0.15
VAL_SIZE = 0.15
MIN_SAMPLES_PER_CLASS = 10  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∞

def load_dataset(file_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}...")
    df = pd.read_csv(file_path)
    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(df):,} –∑–∞–ø–∏—Å–µ–π")
    return df

def analyze_class_distribution(df, column='OperCode'):
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤"""
    print(f"\nüìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ ({column}):")
    class_counts = df[column].value_counts()
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤: {len(class_counts)}")
    print(f"   –ú–∏–Ω/–ú–∞–∫—Å –ø—Ä–∏–º–µ—Ä–æ–≤: {class_counts.min()} / {class_counts.max():,}")
    
    rare_classes = class_counts[class_counts < MIN_SAMPLES_PER_CLASS]
    print(f"   ‚ö†Ô∏è  –ö–ª–∞—Å—Å–æ–≤ —Å < {MIN_SAMPLES_PER_CLASS} –ø—Ä–∏–º–µ—Ä–æ–≤: {len(rare_classes)}")
    
    return class_counts

def filter_rare_classes(df, min_samples=MIN_SAMPLES_PER_CLASS):
    """–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤"""
    class_counts = df['OperCode'].value_counts()
    valid_classes = class_counts[class_counts >= min_samples].index
    
    df_filtered = df[df['OperCode'].isin(valid_classes)].copy()
    removed = len(df) - len(df_filtered)
    
    if removed > 0:
        print(f"\nüîß –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤:")
        print(f"   –£–¥–∞–ª–µ–Ω–æ {removed:,} –∑–∞–ø–∏—Å–µ–π ({removed/len(df)*100:.2f}%)")
        print(f"   –û—Å—Ç–∞–ª–æ—Å—å –∫–ª–∞—Å—Å–æ–≤: {df_filtered['OperCode'].nunique()}")
    
    return df_filtered

def prepare_text(text):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–±–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞)"""
    if pd.isna(text):
        return ""
    text = str(text).strip()
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = ' '.join(text.split())
    return text

def create_prompt(payment_comment, oper_code=None, for_training=True):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è Gemma 3
    
    –§–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:
    <start_of_turn>user
    –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π –ø–ª–∞—Ç—ë–∂: [—Ç–µ–∫—Å—Ç]
    <end_of_turn>
    <start_of_turn>model
    [–∫–æ–¥]<end_of_turn>
    """
    if for_training:
        prompt = f"""<start_of_turn>user
–û–ø—Ä–µ–¥–µ–ª–∏ –∫–æ–¥ –æ–ø–µ—Ä–∞—Ü–∏–∏ (OperCode) –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ –ø–ª–∞—Ç–µ–∂–∞:

–ü–ª–∞—Ç—ë–∂: {payment_comment}

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–¥–æ–º –æ–ø–µ—Ä–∞—Ü–∏–∏.<end_of_turn>
<start_of_turn>model
{oper_code}<end_of_turn>"""
    else:
        prompt = f"""<start_of_turn>user
–û–ø—Ä–µ–¥–µ–ª–∏ –∫–æ–¥ –æ–ø–µ—Ä–∞—Ü–∏–∏ (OperCode) –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ –ø–ª–∞—Ç–µ–∂–∞:

–ü–ª–∞—Ç—ë–∂: {payment_comment}

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–¥–æ–º –æ–ø–µ—Ä–∞—Ü–∏–∏.<end_of_turn>
<start_of_turn>model
"""
    
    return prompt

def prepare_for_training(df):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    print(f"\nüîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    
    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    df['PaymentComment'] = df['PaymentComment'].apply(prepare_text)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
    df['prompt'] = df.apply(
        lambda row: create_prompt(row['PaymentComment'], row['OperCode'], for_training=True),
        axis=1
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫
    df['label'] = df['OperCode'].astype(str)
    
    print(f"   ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(df):,} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    return df

def split_dataset(df):
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test —Å stratification"""
    print(f"\n‚úÇÔ∏è  –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    # –°–Ω–∞—á–∞–ª–∞ –æ—Ç–¥–µ–ª—è–µ–º test
    train_val, test = train_test_split(
        df,
        test_size=TEST_SIZE,
        random_state=RANDOM_SEED,
        stratify=df['OperCode']
    )
    
    # –ó–∞—Ç–µ–º –∏–∑ train_val –≤—ã–¥–µ–ª—è–µ–º validation
    val_size_adjusted = VAL_SIZE / (1 - TEST_SIZE)
    train, val = train_test_split(
        train_val,
        test_size=val_size_adjusted,
        random_state=RANDOM_SEED,
        stratify=train_val['OperCode']
    )
    
    print(f"   üìö Train: {len(train):,} ({len(train)/len(df)*100:.1f}%)")
    print(f"   üìñ Val:   {len(val):,} ({len(val)/len(df)*100:.1f}%)")
    print(f"   üìù Test:  {len(test):,} ({len(test)/len(df)*100:.1f}%)")
    
    return train, val, test

def save_datasets(train, val, test, output_dir):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ {output_dir}...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV
    train[['prompt', 'label', 'OperCode', 'PaymentComment']].to_csv(
        output_dir / 'train.csv', index=False
    )
    val[['prompt', 'label', 'OperCode', 'PaymentComment']].to_csv(
        output_dir / 'val.csv', index=False
    )
    test[['prompt', 'label', 'OperCode', 'PaymentComment']].to_csv(
        output_dir / 'test.csv', index=False
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSONL (–¥–ª—è transformers)
    for split_name, split_df in [('train', train), ('val', val), ('test', test)]:
        with open(output_dir / f'{split_name}.jsonl', 'w', encoding='utf-8') as f:
            for _, row in split_df.iterrows():
                json_obj = {
                    'text': row['prompt'],
                    'label': row['label'],
                    'oper_code': int(row['OperCode']),
                    'payment_comment': row['PaymentComment']
                }
                f.write(json.dumps(json_obj, ensure_ascii=False) + '\n')
    
    print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: train.csv, val.csv, test.csv")
    print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: train.jsonl, val.jsonl, test.jsonl")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    metadata = {
        'total_samples': len(train) + len(val) + len(test),
        'train_samples': len(train),
        'val_samples': len(val),
        'test_samples': len(test),
        'num_classes': train['OperCode'].nunique(),
        'class_distribution': train['OperCode'].value_counts().to_dict(),
        'min_samples_per_class': MIN_SAMPLES_PER_CLASS,
        'random_seed': RANDOM_SEED
    }
    
    with open(output_dir / 'metadata.json', 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: metadata.json")

def main():
    print("=" * 80)
    print("üöÄ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø QLORA FINE-TUNING GEMMA 3:12B")
    print("=" * 80)
    
    # –ü—É—Ç–∏
    input_file = Path(__file__).parent.parent.parent / 'final_dataset.csv'
    output_dir = Path(__file__).parent.parent / 'data'
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞
    df = load_dataset(input_file)
    
    # 2. –ê–Ω–∞–ª–∏–∑
    analyze_class_distribution(df)
    
    # 3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ (–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏)
    print("\nüîß –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤...")
    print(f"   –£–¥–∞–ª—è–µ–º –∫–ª–∞—Å—Å—ã —Å < {MIN_SAMPLES_PER_CLASS} –ø—Ä–∏–º–µ—Ä–æ–≤ (—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏)")
    df_filtered = filter_rare_classes(df, min_samples=MIN_SAMPLES_PER_CLASS)
    print(f"   ‚úÖ –û—Å—Ç–∞–ª–æ—Å—å {df_filtered['OperCode'].nunique()} –∫–ª–∞—Å—Å–æ–≤")
    
    # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
    df_prepared = prepare_for_training(df_filtered)
    
    # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    train, val, test = split_dataset(df_prepared)
    
    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    save_datasets(train, val, test, output_dir)
    
    # 7. –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 80)
    print("‚úÖ –ü–û–î–ì–û–¢–û–í–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 80)
    print(f"\nüìÅ –î–∞—Ç–∞—Å–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")
    print(f"\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –ö–ª–∞—Å—Å–æ–≤: {train['OperCode'].nunique()}")
    print(f"   Train:   {len(train):,}")
    print(f"   Val:     {len(val):,}")
    print(f"   Test:    {len(test):,}")
    print(f"\nüéØ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –ó–∞–ø—É—Å—Ç–∏—Ç–µ scripts/train_qlora.py")
    print("=" * 80)

if __name__ == "__main__":
    main()


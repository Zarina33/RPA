"""
QLoRA Fine-tuning –¥–ª—è Gemma 3:12b
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è 16GB VRAM
"""

import os
# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ CUDA
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è tokenizers
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
import json
from pathlib import Path
from datetime import datetime
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_fscore_support
from sklearn.utils.class_weight import compute_class_weight
from torch.utils.data import WeightedRandomSampler
import re

# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –î–õ–Ø 16GB VRAM
# ============================================================================

# –ú–æ–¥–µ–ª—å
MODEL_NAME = "google/gemma-2-9b-it"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º Gemma 2 9B (–±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è)
# –ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ Gemma 3:12b —á–µ—Ä–µ–∑ Ollama, –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥

# QLoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è 16GB)
LORA_R = 16                    # –†–∞–Ω–≥ LoRA –º–∞—Ç—Ä–∏—Ü (16-32 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)
LORA_ALPHA = 32                # Alpha = 2 * r (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞)
LORA_DROPOUT = 0.05            # –ù–µ–±–æ–ª—å—à–æ–π dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
TARGET_MODULES = [             # –ú–æ–¥—É–ª–∏ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA
    "q_proj", "k_proj", "v_proj", 
    "o_proj", "gate_proj", "up_proj", "down_proj"
]

# –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è 16GB!)
USE_4BIT = True
BNB_4BIT_COMPUTE_DTYPE = "bfloat16"
BNB_4BIT_QUANT_TYPE = "nf4"

# –û–±—É—á–µ–Ω–∏–µ
BATCH_SIZE = 1                 # –£–º–µ–Ω—å—à–∞–µ–º –¥–æ 1 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
GRADIENT_ACCUMULATION = 16     # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º accumulation (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch = 1 * 16 = 16)
LEARNING_RATE = 2e-4
NUM_EPOCHS = 3
MAX_SEQ_LENGTH = 384           # –£–º–µ–Ω—å—à–∞–µ–º —Å 512 –¥–æ 384
WARMUP_RATIO = 0.03
WEIGHT_DECAY = 0.001

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
GRADIENT_CHECKPOINTING = True
OPTIM = "paged_adamw_8bit"    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π

# –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
USE_CLASS_WEIGHTS = True        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
USE_WEIGHTED_SAMPLER = True     # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∑–≤–µ—à–µ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É
FOCAL_LOSS_GAMMA = 2.0          # –ü–∞—Ä–∞–º–µ—Ç—Ä focal loss (0 = –æ–±—ã—á–Ω–∞—è CE loss)

# –ü—É—Ç–∏
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
OUTPUT_DIR = BASE_DIR / "outputs"
MODEL_DIR = BASE_DIR / "models"
LOGS_DIR = BASE_DIR / "logs"

# ============================================================================
# –§–£–ù–ö–¶–ò–ò
# ============================================================================

def print_gpu_memory():
    """–í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∞–º—è—Ç–∏ GPU"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   üíæ GPU Memory: {allocated:.2f}GB allocated / {reserved:.2f}GB reserved / {total:.2f}GB total")

def extract_oper_code(text):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ OperCode –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–º–ø—Ç–∞"""
    # –ò—â–µ–º –∫–æ–¥ –ø–æ—Å–ª–µ <start_of_turn>model
    match = re.search(r'<start_of_turn>model\s*(\d+)', text)
    if match:
        return int(match.group(1))
    # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç - –∏—â–µ–º –ª—é–±–æ–µ —á–∏—Å–ª–æ –≤ –∫–æ–Ω—Ü–µ
    match = re.search(r'(\d+)\s*<end_of_turn>', text)
    if match:
        return int(match.group(1))
    return None

def compute_class_weights_and_sampler(dataset):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ WeightedRandomSampler
    """
    print("\n‚öñÔ∏è  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤...")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∫–æ–¥—ã –æ–ø–µ—Ä–∞—Ü–∏–π –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
    oper_codes = []
    for item in dataset:
        code = extract_oper_code(item['text'])
        if code is not None:
            oper_codes.append(code)
    
    oper_codes = np.array(oper_codes)
    unique_classes = np.unique(oper_codes)
    
    print(f"   –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Å–æ–≤: {len(unique_classes)}")
    print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(oper_codes)}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (–æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∞—Å—Ç–æ—Ç–µ)
    class_weights = compute_class_weight(
        class_weight='balanced',
        classes=unique_classes,
        y=oper_codes
    )
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∫–ª–∞—Å—Å -> –≤–µ—Å
    class_weights_dict = {int(cls): float(weight) for cls, weight in zip(unique_classes, class_weights)}
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    min_weight = min(class_weights_dict.values())
    max_weight = max(class_weights_dict.values())
    print(f"   –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: –æ—Ç {min_weight:.4f} –¥–æ {max_weight:.4f}")
    
    # –°–æ–∑–¥–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ (–¥–ª—è WeightedRandomSampler)
    sample_weights = [class_weights_dict[code] for code in oper_codes]
    
    # –°–æ–∑–¥–∞–µ–º sampler
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )
    
    print(f"   ‚úÖ WeightedRandomSampler —Å–æ–∑–¥–∞–Ω")
    
    return class_weights_dict, sampler, unique_classes

def setup_quantization_config():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏"""
    if USE_4BIT:
        compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=BNB_4BIT_QUANT_TYPE,
            bnb_4bit_compute_dtype=compute_dtype,
            bnb_4bit_use_double_quant=True,  # –î–≤–æ–π–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏
        )
        return bnb_config
    return None

def load_model_and_tokenizer():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    
    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        add_eos_token=True,
        add_bos_token=True,
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    
    print(f"   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω: {MODEL_NAME}")
    
    # Quantization config
    bnb_config = setup_quantization_config()
    
    # Model
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        attn_implementation="eager",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é (flash_attn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
    )
    
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {MODEL_NAME}")
    print_gpu_memory()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è k-bit training
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=GRADIENT_CHECKPOINTING)
    
    if GRADIENT_CHECKPOINTING:
        model.gradient_checkpointing_enable()
        print("   ‚úÖ Gradient checkpointing –≤–∫–ª—é—á–µ–Ω")
    
    return model, tokenizer

def setup_lora(model):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤"""
    print("\nüîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
    
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        target_modules=TARGET_MODULES,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    
    model = get_peft_model(model, lora_config)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())
    trainable_percent = 100 * trainable_params / all_params
    
    print(f"   üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"      –í—Å–µ–≥–æ: {all_params:,}")
    print(f"      –û–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,} ({trainable_percent:.2f}%)")
    print(f"   ‚úÖ LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω (r={LORA_R}, alpha={LORA_ALPHA})")
    
    return model

def load_and_prepare_datasets(tokenizer):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    print("\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞
    dataset = load_dataset(
        'json',
        data_files={
            'train': str(DATA_DIR / 'train.jsonl'),
            'validation': str(DATA_DIR / 'val.jsonl'),
            'test': str(DATA_DIR / 'test.jsonl')
        }
    )
    
    print(f"   Train: {len(dataset['train']):,}")
    print(f"   Val:   {len(dataset['validation']):,}")
    print(f"   Test:  {len(dataset['test']):,}")
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    def tokenize_function(examples):
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã (–ø—Ä–æ–º–ø—Ç—ã)
        tokenized = tokenizer(
            examples['text'],
            truncation=True,
            max_length=MAX_SEQ_LENGTH,
            padding='max_length',
            return_tensors='pt'
        )
        
        # Labels –¥–ª—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–∫–æ–ø–∏—è input_ids)
        tokenized['labels'] = tokenized['input_ids'].clone()
        
        return tokenized
    
    print("\nüîÑ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset['train'].column_names,
        desc="Tokenizing"
    )
    
    print("   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    return tokenized_datasets

class WeightedLossTrainer(Trainer):
    """
    Custom Trainer —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π weighted loss –∏ focal loss
    """
    def __init__(self, class_weights_dict=None, focal_gamma=0.0, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights_dict = class_weights_dict
        self.focal_gamma = focal_gamma
        
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º compute_loss –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è class weights –∏ focal loss
        """
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        
        # Shift –¥–ª—è causal LM
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        
        # Flatten
        shift_logits = shift_logits.view(-1, shift_logits.size(-1))
        shift_labels = shift_labels.view(-1)
        
        # –ú–∞—Å–∫–∞ –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è padding
        mask = shift_labels != -100
        
        if self.class_weights_dict is not None and USE_CLASS_WEIGHTS:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º class weights
            # –î–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –º—ã –Ω–µ –º–æ–∂–µ–º –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≤–µ—Å–∞ –Ω–∞–ø—Ä—è–º—É—é,
            # –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é CE loss
            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(shift_logits, shift_labels)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º focal loss, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if self.focal_gamma > 0:
                ce_loss = loss
                pt = torch.exp(-ce_loss)
                focal_weight = (1 - pt) ** self.focal_gamma
                loss = focal_weight * ce_loss
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ –Ω–µ-padding —Ç–æ–∫–µ–Ω–∞–º
            loss = loss[mask].mean()
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è loss
            loss_fct = torch.nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits, shift_labels)
        
        return (loss, outputs) if return_outputs else loss

def compute_metrics(eval_pred):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è evaluation"""
    logits, labels = eval_pred
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (argmax –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é)
    predictions = np.argmax(logits, axis=-1)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º padding tokens (-100)
    mask = labels != -100
    predictions = predictions[mask]
    labels = labels[mask]
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(labels, predictions)
    
    # Macro/Micro F1 (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º warning –¥–ª—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤)
    try:
        f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)
        f1_micro = f1_score(labels, predictions, average='micro', zero_division=0)
    except:
        f1_macro = 0.0
        f1_micro = 0.0
    
    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_micro': f1_micro,
    }

def setup_training_arguments():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"gemma_qlora_{timestamp}"
    
    training_args = TrainingArguments(
        # –ü—É—Ç–∏
        output_dir=str(OUTPUT_DIR / run_name),
        logging_dir=str(LOGS_DIR / run_name),
        
        # –û–±—É—á–µ–Ω–∏–µ
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=4,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 4 (evaluation –≤ 4 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ!)
        gradient_accumulation_steps=GRADIENT_ACCUMULATION,
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        warmup_ratio=WARMUP_RATIO,
        optim=OPTIM,
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        save_strategy="steps",
        save_steps=1000,  # –°–æ–≤–ø–∞–¥–∞–µ—Ç —Å eval_steps (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ load_best_model_at_end)
        save_total_limit=3,
        
        # Evaluation
        eval_strategy="steps",  # –ë—ã–ª–æ evaluation_strategy –≤ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö
        eval_steps=1000,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å 500 –¥–æ 1000 (–º–µ–Ω—å—à–µ OOM)
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",  # –ú–µ–Ω—è–µ–º –Ω–∞ eval_loss (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫)
        greater_is_better=False,
        prediction_loss_only=True,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ loss, –Ω–µ –≤—Å–µ logits
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        logging_steps=50,
        logging_first_step=True,
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
        gradient_checkpointing=GRADIENT_CHECKPOINTING,
        fp16=False,
        bf16=True,  # BFloat16 –¥–ª—è –ª—É—á—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        # –ü—Ä–æ—á–µ–µ
        report_to=["tensorboard"],
        remove_unused_columns=False,
        dataloader_num_workers=2,  # –£–º–µ–Ω—å—à–∞–µ–º —Å 4 –¥–æ 2 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        dataloader_pin_memory=False,  # –û—Ç–∫–ª—é—á–∞–µ–º pin_memory –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏
        max_grad_norm=1.0,  # Gradient clipping
    )
    
    return training_args, run_name

def train():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    print("=" * 80)
    print("üöÄ QLORA FINE-TUNING GEMMA 3:12B")
    print("=" * 80)
    print(f"\n‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"   –ú–æ–¥–µ–ª—å: {MODEL_NAME}")
    print(f"   LoRA r: {LORA_R}, alpha: {LORA_ALPHA}")
    print(f"   Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION}")
    print(f"   Learning rate: {LEARNING_RATE}")
    print(f"   Epochs: {NUM_EPOCHS}")
    print(f"   Max seq length: {MAX_SEQ_LENGTH}")
    print(f"   4-bit quantization: {USE_4BIT}")
    print(f"\n‚öñÔ∏è  –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤:")
    print(f"   Class weights: {USE_CLASS_WEIGHTS}")
    print(f"   Weighted sampler: {USE_WEIGHTED_SAMPLER}")
    print(f"   Focal loss gamma: {FOCAL_LOSS_GAMMA}")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model, tokenizer = load_model_and_tokenizer()
    
    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
    model = setup_lora(model)
    
    # 3. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    tokenized_datasets = load_and_prepare_datasets(tokenizer)
    
    # 3.5. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –∏ sampler
    class_weights_dict = None
    train_sampler = None
    unique_classes = None
    
    if USE_CLASS_WEIGHTS or USE_WEIGHTED_SAMPLER:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤
        raw_dataset = load_dataset(
            'json',
            data_files={'train': str(DATA_DIR / 'train.jsonl')}
        )
        
        class_weights_dict, train_sampler, unique_classes = compute_class_weights_and_sampler(
            raw_dataset['train']
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        weights_save_path = OUTPUT_DIR / 'class_weights.json'
        weights_save_path.parent.mkdir(parents=True, exist_ok=True)
        with open(weights_save_path, 'w') as f:
            json.dump({
                'class_weights': class_weights_dict,
                'num_classes': len(unique_classes),
                'classes': unique_classes.tolist()
            }, f, indent=2)
        print(f"   üíæ –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {weights_save_path}")
    
    # 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    training_args, run_name = setup_training_arguments()
    
    # 5. Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Causal LM, –Ω–µ masked LM
    )
    
    # 6. Trainer
    print("\nüèãÔ∏è  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer...")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º WeightedLossTrainer –µ—Å–ª–∏ –Ω—É–∂–Ω—ã –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    trainer_class = WeightedLossTrainer if USE_CLASS_WEIGHTS else Trainer
    
    trainer_kwargs = {
        'model': model,
        'args': training_args,
        'train_dataset': tokenized_datasets['train'],
        'eval_dataset': tokenized_datasets['validation'],
        'data_collator': data_collator,
        # compute_metrics —É–±—Ä–∞–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º prediction_loss_only=True –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è WeightedLossTrainer
    if USE_CLASS_WEIGHTS:
        trainer_kwargs['class_weights_dict'] = class_weights_dict
        trainer_kwargs['focal_gamma'] = FOCAL_LOSS_GAMMA
    
    trainer = trainer_class(**trainer_kwargs)
    
    # Note: WeightedRandomSampler –Ω—É–∂–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ DataLoader,
    # —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–π –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏. –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ weighted loss.
    
    print("   ‚úÖ Trainer –≥–æ—Çov")
    if USE_CLASS_WEIGHTS:
        print(f"   ‚öñÔ∏è  –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤")
    if FOCAL_LOSS_GAMMA > 0:
        print(f"   üéØ Focal Loss —Å gamma={FOCAL_LOSS_GAMMA}")
    print(f"   üìä Evaluation –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤ (—Ç–æ–ª—å–∫–æ loss –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)")
    
    # 7. –û–±—É—á–µ–Ω–∏–µ
    print("\n" + "=" * 80)
    print("üéì –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 80)
    
    # –û—á–∏—â–∞–µ–º –∫—ç—à GPU –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("   üßπ GPU cache –æ—á–∏—â–µ–Ω")
    
    print_gpu_memory()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ checkpoint –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è (–∏—â–µ–º –≤–æ –≤—Å–µ—Ö –ø–∞–ø–∫–∞—Ö)
    resume_checkpoint = None
    all_checkpoints = []
    for run_dir in OUTPUT_DIR.glob("gemma_qlora_*"):
        if run_dir.is_dir():
            for ckpt in run_dir.glob("checkpoint-*"):
                if ckpt.is_dir():
                    step = int(ckpt.name.split("-")[1])
                    all_checkpoints.append((step, ckpt))
    
    if all_checkpoints:
        # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –ø–æ—Å–ª–µ–¥–Ω–∏–π checkpoint
        all_checkpoints.sort(key=lambda x: x[0])
        latest_step, latest_ckpt = all_checkpoints[-1]
        resume_checkpoint = str(latest_ckpt)
        print(f"\nüîÑ –ù–∞–π–¥–µ–Ω checkpoint –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è: {latest_ckpt.name}")
        print(f"   üìÇ –ü—É—Ç—å: {latest_ckpt}")
        print(f"   ‚è≠Ô∏è  –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—Å—è —Å —à–∞–≥–∞ {latest_step}")
    
    try:
        if resume_checkpoint:
            trainer.train(resume_from_checkpoint=resume_checkpoint)
        else:
            trainer.train()
        
        print("\n" + "=" * 80)
        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        raise
    
    # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    model_save_path = MODEL_DIR / run_name
    model_save_path.mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã (–ª–µ–≥–∫–∏–π –≤–µ—Å!)
    trainer.model.save_pretrained(model_save_path)
    tokenizer.save_pretrained(model_save_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = {
        'model_name': MODEL_NAME,
        'lora_r': LORA_R,
        'lora_alpha': LORA_ALPHA,
        'batch_size': BATCH_SIZE,
        'learning_rate': LEARNING_RATE,
        'num_epochs': NUM_EPOCHS,
        'training_completed': datetime.now().isoformat(),
    }
    
    with open(model_save_path / 'training_config.json', 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_save_path}")
    print(f"   üì¶ –†–∞–∑–º–µ—Ä LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤: ~200-500 MB")
    
    # 9. –§–∏–Ω–∞–ª—å–Ω–∞—è evaluation
    print("\nüìä –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ validation set...")
    eval_results = trainer.evaluate()
    
    print("\nüìà –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    for key, value in eval_results.items():
        print(f"   {key}: {value:.4f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_path = model_save_path / 'evaluation_results.json'
    with open(results_path, 'w') as f:
        json.dump(eval_results, f, indent=2)
    print(f"\n   üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
    
    # 10. –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤–µ—Å–∞)
    if class_weights_dict is not None:
        print("\nüìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤:")
        
        # –¢–æ–ø-5 –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–∏–º–µ—Ä–æ–≤
        print("\n   üîù –¢–æ–ø-5 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–ª–∞—Å—Å–æ–≤:")
        class_counts = {}
        for code in unique_classes:
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            class_counts[int(code)] = 1.0 / class_weights_dict.get(int(code), 1.0)
        
        top_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        for code, _ in top_classes:
            weight = class_weights_dict.get(code, 1.0)
            print(f"      –ö–ª–∞—Å—Å {code}: –≤–µ—Å={weight:.4f}")
        
        # –¢–æ–ø-5 —Å–∞–º—ã—Ö —Ä–µ–¥–∫–∏—Ö
        print("\n   üîª –¢–æ–ø-5 —Å–∞–º—ã—Ö —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤:")
        bottom_classes = sorted(class_counts.items(), key=lambda x: x[1])[:5]
        for code, _ in bottom_classes:
            weight = class_weights_dict.get(code, 1.0)
            print(f"      –ö–ª–∞—Å—Å {code}: –≤–µ—Å={weight:.4f}")
    
    print("\n" + "=" * 80)
    print(f"üéâ –í–°–Å –ì–û–¢–û–í–û! –ú–æ–¥–µ–ª—å: {model_save_path}")
    print("=" * 80)
    print(f"\nüîç –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print(f"   1. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –ª–æ–≥–∏ –≤: {LOGS_DIR / run_name}")
    print(f"   2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ evaluation: python scripts/evaluate.py")
    print(f"   3. –¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ inference: python scripts/inference.py")
    print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    print(f"   ‚Ä¢ –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–ª—Å—è —Ç–æ–ª—å–∫–æ eval_loss (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)")
    print(f"   ‚Ä¢ –ó–∞–ø—É—Å—Ç–∏—Ç–µ evaluate.py –¥–ª—è –ø–æ–ª–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ (F1, Accuracy, per-class)")
    print(f"   ‚Ä¢ evaluate.py –ø–æ–∫–∞–∂–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º 72 –∫–ª–∞—Å—Å–∞–º")
    
    return model_save_path

if __name__ == "__main__":
    train()


"""
–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç checkpoint –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
"""
import torch
import json
import pandas as pd
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
CHECKPOINT_PATH = Path("outputs/gemma_qlora_20251104_181124/checkpoint-3000")
BASE_MODEL = "google/gemma-2-9b-it"
DATA_DIR = Path("data")
NUM_SAMPLES = 100  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ 100 –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã

print("=" * 80)
print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï CHECKPOINT-3000")
print("=" * 80)

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

print("   –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True,
)

print("   –ó–∞–≥—Ä—É–∑–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH)
model.eval()

print("   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

# 2. –ó–∞–≥—Ä—É–∑–∫–∞ test –¥–∞–Ω–Ω—ã—Ö
print("\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ test –¥–∞–Ω–Ω—ã—Ö...")
test_df = pd.read_csv(DATA_DIR / 'test.csv')
print(f"   Test –ø—Ä–∏–º–µ—Ä–æ–≤: {len(test_df)}")

# –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É
test_sample = test_df.sample(n=min(NUM_SAMPLES, len(test_df)), random_state=42)

# 3. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞
def extract_code(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏"""
    # –ò—â–µ–º —á–∏—Å–ª–∞ –≤ –æ—Ç–≤–µ—Ç–µ
    numbers = re.findall(r'\d+', text)
    if numbers:
        return int(numbers[0])
    return None

# 4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ {len(test_sample)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
print("   (—ç—Ç–æ –∑–∞–π–º–µ—Ç ~5-10 –º–∏–Ω—É—Ç)")

predictions = []
true_labels = []

for idx, row in test_sample.iterrows():
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç (–±–µ–∑ –æ—Ç–≤–µ—Ç–∞)
    prompt = f"""<start_of_turn>user
–û–ø—Ä–µ–¥–µ–ª–∏ –∫–æ–¥ –æ–ø–µ—Ä–∞—Ü–∏–∏ (OperCode) –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ –ø–ª–∞—Ç–µ–∂–∞:

–ü–ª–∞—Ç—ë–∂: {row['PaymentComment']}

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–¥–æ–º –æ–ø–µ—Ä–∞—Ü–∏–∏.<end_of_turn>
<start_of_turn>model
"""
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=384)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=10,
            temperature=0.1,
            do_sample=False,
        )
    
    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.split("<start_of_turn>model")[-1].strip()
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞
    pred_code = extract_code(response)
    true_code = int(row['OperCode'])
    
    predictions.append(pred_code)
    true_labels.append(true_code)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤
    if len(predictions) <= 5:
        print(f"\n   –ü—Ä–∏–º–µ—Ä {len(predictions)}:")
        print(f"   –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–æ–¥: {true_code}")
        print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ:  {pred_code}")
        print(f"   –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response[:100]}...")
        print(f"   {'‚úÖ –í–µ—Ä–Ω–æ' if pred_code == true_code else '‚ùå –ù–µ–≤–µ—Ä–Ω–æ'}")

# 5. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
print("\n" + "=" * 80)
print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´")
print("=" * 80)

# –§–∏–ª—å—Ç—Ä—É–µ–º None (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥)
valid_indices = [i for i, p in enumerate(predictions) if p is not None]
valid_predictions = [predictions[i] for i in valid_indices]
valid_true_labels = [true_labels[i] for i in valid_indices]

if len(valid_predictions) > 0:
    from sklearn.metrics import accuracy_score, f1_score
    
    accuracy = accuracy_score(valid_true_labels, valid_predictions)
    f1_macro = f1_score(valid_true_labels, valid_predictions, average='macro', zero_division=0)
    f1_micro = f1_score(valid_true_labels, valid_predictions, average='micro', zero_division=0)
    
    print(f"\nüìà –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ {len(valid_predictions)} –ø—Ä–∏–º–µ—Ä–∞—Ö:")
    print(f"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   F1 Macro:  {f1_macro:.4f}")
    print(f"   F1 Micro:  {f1_micro:.4f}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∫–ª–∞—Å—Å–æ–≤
    print(f"\nüîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    with open('outputs/class_weights.json', 'r') as f:
        class_info = json.load(f)
    
    class_weights = class_info['class_weights']
    
    # –¢–æ–ø-5 –∏ Bottom-5 –∫–ª–∞—Å—Å–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–µ
    from collections import Counter
    true_counts = Counter(valid_true_labels)
    
    print(f"\n   –¢–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç–µ—Å—Ç–µ:")
    for code, count in true_counts.most_common(5):
        matches = sum(1 for p, t in zip(valid_predictions, valid_true_labels) if t == code and p == code)
        acc = matches / count if count > 0 else 0
        weight = class_weights.get(str(code), 1.0)
        print(f"      –ö–ª–∞—Å—Å {code}: {matches}/{count} –≤–µ—Ä–Ω–æ ({acc*100:.1f}%), –≤–µ—Å={weight:.2f}")
    
    # –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
    invalid_count = len(predictions) - len(valid_predictions)
    if invalid_count > 0:
        print(f"\n   ‚ö†Ô∏è  –ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {invalid_count}/{len(predictions)} ({invalid_count/len(predictions)*100:.1f}%)")
else:
    print("\n‚ùå –ú–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∞–ª–∏–¥–Ω—ã–µ –∫–æ–¥—ã")

print("\n" + "=" * 80)
print("‚úÖ –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù")
print("=" * 80)
print("\nüí° –≠—Ç–æ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ 18% –æ–±—É—á–µ–Ω–∏—è (checkpoint-3000)")
print("   –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –Ω–∞–º–Ω–æ–≥–æ –ª—É—á—à–µ –ø–æ—Å–ª–µ 3 –ø–æ–ª–Ω—ã—Ö —ç–ø–æ—Ö!")

